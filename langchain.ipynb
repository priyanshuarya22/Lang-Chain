{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNXLwf9bP8WLXcYfr09tuBj"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Model I/O: Loading Quantized Models with LangChain","metadata":{}},{"cell_type":"code","source":"!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:00:41.888894Z","iopub.execute_input":"2024-10-29T10:00:41.889758Z","iopub.status.idle":"2024-10-29T10:03:45.377446Z","shell.execute_reply.started":"2024-10-29T10:00:41.889710Z","shell.execute_reply":"2024-10-29T10:03:45.376485Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"--2024-10-29 10:00:42--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\nResolving huggingface.co (huggingface.co)... 18.172.134.124, 18.172.134.4, 18.172.134.24, ...\nConnecting to huggingface.co (huggingface.co)|18.172.134.124|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1730455242&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMDQ1NTI0Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Pjg9ss%7EtZV4sXAgnLSuIQHojFLCVgZHOMvRTZtsvTwGXexK3gdAA9Ttij9vv1U6za8oC%7EmXLnLyOfUJOYQRTK8NQSzIR9UxltqwnU2RDzJFwnOcz4oOCRPGIkGIyNJW6fZLmINy2RIp%7EzmXZzWHcFxWFemqxe6sPhkIZl3WSl5%7EP0ASQ1BtVbdhi5JKXoYAZM9iwrmUVkBD83C3ug5siC4hdjPQVxTfFUvCZO809dP9GzDHnxid8S2f%7EJHUrImzDIPXq5skPv5zzCHpNuSTEGXAADlfeoBdB7lz0xmqHb1ypSq6tscmhuCQMUJ9OMlwXJAAWim7cEzlUjJa0rd9a%7Ew__&Key-Pair-Id=K24J24Z295AEI9 [following]\n--2024-10-29 10:00:42--  https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8''Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1730455242&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMDQ1NTI0Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Pjg9ss~tZV4sXAgnLSuIQHojFLCVgZHOMvRTZtsvTwGXexK3gdAA9Ttij9vv1U6za8oC~mXLnLyOfUJOYQRTK8NQSzIR9UxltqwnU2RDzJFwnOcz4oOCRPGIkGIyNJW6fZLmINy2RIp~zmXZzWHcFxWFemqxe6sPhkIZl3WSl5~P0ASQ1BtVbdhi5JKXoYAZM9iwrmUVkBD83C3ug5siC4hdjPQVxTfFUvCZO809dP9GzDHnxid8S2f~JHUrImzDIPXq5skPv5zzCHpNuSTEGXAADlfeoBdB7lz0xmqHb1ypSq6tscmhuCQMUJ9OMlwXJAAWim7cEzlUjJa0rd9a~w__&Key-Pair-Id=K24J24Z295AEI9\nResolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.160.225.13, 18.160.225.98, 18.160.225.120, ...\nConnecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.160.225.13|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7643295904 (7.1G) [binary/octet-stream]\nSaving to: 'Phi-3-mini-4k-instruct-fp16.gguf'\n\nPhi-3-mini-4k-instr 100%[===================>]   7.12G  39.4MB/s    in 3m 2s   \n\n2024-10-29 10:03:45 (40.0 MB/s) - 'Phi-3-mini-4k-instruct-fp16.gguf' saved [7643295904/7643295904]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install langchain","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:03:52.984016Z","iopub.execute_input":"2024-10-29T10:03:52.984422Z","iopub.status.idle":"2024-10-29T10:04:09.902975Z","shell.execute_reply.started":"2024-10-29T10:03:52.984381Z","shell.execute_reply":"2024-10-29T10:04:09.901912Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.4.0,>=0.3.12 (from langchain)\n  Downloading langchain_core-0.3.13-py3-none-any.whl.metadata (6.3 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.137-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.9.2)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\nCollecting packaging<25,>=23.2 (from langchain-core<0.4.0,>=0.3.12->langchain)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.4)\nCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (2.4)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.0)\nDownloading langchain-0.3.4-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.13-py3-none-any.whl (408 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.0/408.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.137-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.9/296.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, requests-toolbelt, langsmith, langchain-core, langchain-text-splitters, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: requests-toolbelt\n    Found existing installation: requests-toolbelt 0.10.1\n    Uninstalling requests-toolbelt-0.10.1:\n      Successfully uninstalled requests-toolbelt-0.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.3.4 langchain-core-0.3.13 langchain-text-splitters-0.3.0 langsmith-0.1.137 packaging-24.1 requests-toolbelt-1.0.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install langchain_community","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:04:45.829699Z","iopub.execute_input":"2024-10-29T10:04:45.830482Z","iopub.status.idle":"2024-10-29T10:05:00.066356Z","shell.execute_reply.started":"2024-10-29T10:04:45.830441Z","shell.execute_reply":"2024-10-29T10:05:00.065256Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting langchain_community\n  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (3.9.5)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.6.7)\nRequirement already satisfied: langchain<0.4.0,>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.3.4)\nRequirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.3.13)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.125 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.1.137)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (1.26.4)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n  Downloading pydantic_settings-2.6.0-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (8.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.4->langchain_community) (0.3.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.4->langchain_community) (2.9.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.12->langchain_community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.12->langchain_community) (24.1)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.12->langchain_community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.4)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\nRequirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain_community) (2.4)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain_community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain_community) (2.23.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.0)\nDownloading langchain_community-0.3.3-py3-none-any.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.6.0-py3-none-any.whl (28 kB)\nInstalling collected packages: pydantic-settings, langchain_community\nSuccessfully installed langchain_community-0.3.3 pydantic-settings-2.6.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install llama-cpp-python","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:05:07.904731Z","iopub.execute_input":"2024-10-29T10:05:07.905497Z","iopub.status.idle":"2024-10-29T10:07:01.117972Z","shell.execute_reply.started":"2024-10-29T10:05:07.905456Z","shell.execute_reply":"2024-10-29T10:07:01.116852Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting llama-cpp-python\n  Downloading llama_cpp_python-0.3.1.tar.gz (63.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.1-cp310-cp310-linux_x86_64.whl size=3511089 sha256=41fd05d0483a4a459260a50c00ea15363bf5233ec2d08100fdd73bcb34973c37\n  Stored in directory: /root/.cache/pip/wheels/f8/b0/a2/f47d952aec7ab061b9e2a345e23a1e1e137beb7891259e3d0c\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain import LlamaCpp\n\n# Make sure the model path is correct for your system!\nllm = LlamaCpp(\n    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n    n_gpu_layers=-1,\n    max_tokens=500,\n    n_ctx=2048,\n    seed=42,\n    verbose=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:07:08.389592Z","iopub.execute_input":"2024-10-29T10:07:08.390363Z","iopub.status.idle":"2024-10-29T10:07:10.676645Z","shell.execute_reply.started":"2024-10-29T10:07:08.390320Z","shell.execute_reply":"2024-10-29T10:07:10.675648Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"llm.invoke(\"Hi! My name is Priyanshu. What is 1 + 1?\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:07:14.888933Z","iopub.execute_input":"2024-10-29T10:07:14.889747Z","iopub.status.idle":"2024-10-29T10:07:23.894275Z","shell.execute_reply.started":"2024-10-29T10:07:14.889706Z","shell.execute_reply":"2024-10-29T10:07:23.893374Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'\\n<|assistant|> The answer to 1 + 1 is 2. How can I assist you further?'"},"metadata":{}}]},{"cell_type":"code","source":"%pip install openai","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:07:30.113890Z","iopub.execute_input":"2024-10-29T10:07:30.114565Z","iopub.status.idle":"2024-10-29T10:07:42.688513Z","shell.execute_reply.started":"2024-10-29T10:07:30.114525Z","shell.execute_reply":"2024-10-29T10:07:42.687219Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting openai\n  Downloading openai-1.52.2-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\nCollecting jiter<1,>=0.4.0 (from openai)\n  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.9.2)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.4)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai) (4.12.2)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\nDownloading openai-1.52.2-py3-none-any.whl (386 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: jiter, openai\nSuccessfully installed jiter-0.6.1 openai-1.52.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.chat_models import ChatOpenAI\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\n# Create a chat-based LLM\nchat_model = ChatOpenAI(openai_api_key=user_secrets.get_secret(\"openAIAPIKey\"))","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:07:48.384067Z","iopub.execute_input":"2024-10-29T10:07:48.384435Z","iopub.status.idle":"2024-10-29T10:07:49.214529Z","shell.execute_reply.started":"2024-10-29T10:07:48.384401Z","shell.execute_reply":"2024-10-29T10:07:49.213771Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/945195020.py:7: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n  chat_model = ChatOpenAI(openai_api_key=user_secrets.get_secret(\"openAIAPIKey\"))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"A Single Link in the Chain: Prompt Template","metadata":{}},{"cell_type":"code","source":"from langchain import PromptTemplate\n\n# Create a prompt template with the \"input_prompt\" variable\ntemplate = \"\"\"<s><|user|>\n{input_prompt}<|end|>\n<|assistant|>\"\"\"\nprompt = PromptTemplate(\n    template=template,\n    input_variables=[\"input_prompt\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:18:36.908973Z","iopub.execute_input":"2024-10-29T10:18:36.909383Z","iopub.status.idle":"2024-10-29T10:18:36.914501Z","shell.execute_reply.started":"2024-10-29T10:18:36.909345Z","shell.execute_reply":"2024-10-29T10:18:36.913504Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"basic_chain = prompt | llm","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:19:37.399145Z","iopub.execute_input":"2024-10-29T10:19:37.399529Z","iopub.status.idle":"2024-10-29T10:19:37.404971Z","shell.execute_reply.started":"2024-10-29T10:19:37.399493Z","shell.execute_reply":"2024-10-29T10:19:37.404067Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Use the chain\nbasic_chain.invoke(\n    {\n        \"input_prompt\": \"Hi! My name is Priyanshu. What is 1 + 1?\",\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:21:03.948697Z","iopub.execute_input":"2024-10-29T10:21:03.949533Z","iopub.status.idle":"2024-10-29T10:21:12.318413Z","shell.execute_reply.started":"2024-10-29T10:21:03.949494Z","shell.execute_reply":"2024-10-29T10:21:12.317414Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"' Hello Priyanshu! The answer to 1 + 1 is 2.'"},"metadata":{}}]},{"cell_type":"code","source":"# Create a Chain that creates our business' name\ntemplate = \"Create a funny name for a business that sells {product}.\"\nname_prompt = PromptTemplate(\n    template=template,\n    input_variables=[\"product\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:24:31.164018Z","iopub.execute_input":"2024-10-29T10:24:31.164658Z","iopub.status.idle":"2024-10-29T10:24:31.169077Z","shell.execute_reply.started":"2024-10-29T10:24:31.164617Z","shell.execute_reply":"2024-10-29T10:24:31.168206Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"A Chain with Multiple Prompts","metadata":{}},{"cell_type":"code","source":"from langchain import LLMChain\n\n# Create a chain for the title of our story\ntemplate = \"\"\"<s><|user|>\nCreate a title for a story about {summary}. Only return the title.<|end|>\n<|assistant|>\"\"\"\ntitle_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\ntitle = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:29:53.778824Z","iopub.execute_input":"2024-10-29T10:29:53.779228Z","iopub.status.idle":"2024-10-29T10:29:53.863377Z","shell.execute_reply.started":"2024-10-29T10:29:53.779191Z","shell.execute_reply":"2024-10-29T10:29:53.862475Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3105618174.py:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n  title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n","output_type":"stream"}]},{"cell_type":"code","source":"title.invoke({\"summary\": \"a girl that lost her mother\"})","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:31:31.763978Z","iopub.execute_input":"2024-10-29T10:31:31.764374Z","iopub.status.idle":"2024-10-29T10:31:39.640985Z","shell.execute_reply.started":"2024-10-29T10:31:31.764337Z","shell.execute_reply":"2024-10-29T10:31:39.639997Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'summary': 'a girl that lost her mother',\n 'title': ' \"Whispers of a Lone Star: A Tribute to Mom\"'}"},"metadata":{}}]},{"cell_type":"code","source":"# Create a chain for the character description using the summary and title\ntemplate = \"\"\"<s><|user|>\nDescribe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n<|assistant|>\"\"\"\ncharacter_prompt = PromptTemplate(\n    template=template, input_variables=[\"summary\", \"title\"]\n)\ncharacter = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:35:32.378002Z","iopub.execute_input":"2024-10-29T10:35:32.378865Z","iopub.status.idle":"2024-10-29T10:35:32.384348Z","shell.execute_reply.started":"2024-10-29T10:35:32.378824Z","shell.execute_reply":"2024-10-29T10:35:32.383429Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Create a chain for the story using the summary, title, and character descripiton\ntemplate = \"\"\"<s><|user|>\nCreate a story about {summary} with the title {title}. The main character is: {character}. ONly return the story and it cannot be longer than one paragraph. <|end|>\n<|assistant|>\"\"\"\nstory_prompt = PromptTemplate(\n    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n)\nstory = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:38:59.093708Z","iopub.execute_input":"2024-10-29T10:38:59.094309Z","iopub.status.idle":"2024-10-29T10:38:59.099804Z","shell.execute_reply.started":"2024-10-29T10:38:59.094268Z","shell.execute_reply":"2024-10-29T10:38:59.098747Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Combine all three components to create the full chain\nllm_chain = title | character | story","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:39:43.418779Z","iopub.execute_input":"2024-10-29T10:39:43.419169Z","iopub.status.idle":"2024-10-29T10:39:43.423730Z","shell.execute_reply.started":"2024-10-29T10:39:43.419132Z","shell.execute_reply":"2024-10-29T10:39:43.422781Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"llm_chain.invoke(\"a girl that lost her mother\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:40:15.933779Z","iopub.execute_input":"2024-10-29T10:40:15.934278Z","iopub.status.idle":"2024-10-29T10:42:29.696024Z","shell.execute_reply.started":"2024-10-29T10:40:15.934239Z","shell.execute_reply":"2024-10-29T10:42:29.695033Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'summary': 'a girl that lost her mother',\n 'title': ' \"A Mother\\'s Absence: A Journey Through Loss and Love\"',\n 'character': \" The protagonist, Emily, is a resilient and introspective teenage girl grappling with the overwhelming pain of losing her beloved mother. Her journey through grief leads to an exploration of love, healing, and self-discovery as she navigates life's complexities without her guiding maternal presence.\",\n 'story': \" A Mother's Absence: A Journey Through Loss and Love\\n\\nEmily, a resilient and introspective teenage girl, found herself wrestling with the overwhelming pain of losing her beloved mother to cancer. Her days were now colored by melancholy hues that echoed through empty spaces once filled with laughter and warm embraces. As she traversed the rugged terrain of grief, Emily's heart sought solace in memories stitched with love—the tender kisses on her forehead before bedtime, the reassuring words whispered during stormy nights. With each passing day, a spark within ignited, urging her to embrace life without surrendering to despair. As Emily discovered hidden strength in vulnerability and learned that healing is not about forgetting but cherishing and carrying love forward, she embarked on an introspective journey of self-discovery, illuminating a path where the absence of one mother became a testament to enduring bonds—a living legacy intertwined with her own unfolding story.\"}"},"metadata":{}}]},{"cell_type":"markdown","source":"Memory: Helping LLMs to Remember Conversations","metadata":{}},{"cell_type":"code","source":"# Let's give the LLM our name\nbasic_chain.invoke({\"input_prompt\": \"Hi! My name is Priyanshu. What is 1 + 1?\"})","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:46:09.174285Z","iopub.execute_input":"2024-10-29T10:46:09.174646Z","iopub.status.idle":"2024-10-29T10:46:17.298083Z","shell.execute_reply.started":"2024-10-29T10:46:09.174612Z","shell.execute_reply":"2024-10-29T10:46:17.297106Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"' Hello Priyanshu! The answer to 1 + 1 is 2.'"},"metadata":{}}]},{"cell_type":"code","source":"# Next, we ask the LLM to reproduce the name\nbasic_chain.invoke({\"input_prompt\": \"What is my name?\"})","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:56:25.108390Z","iopub.execute_input":"2024-10-29T10:56:25.108747Z","iopub.status.idle":"2024-10-29T10:56:37.594091Z","shell.execute_reply.started":"2024-10-29T10:56:25.108713Z","shell.execute_reply":"2024-10-29T10:56:37.593125Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"\" I'm unable to determine your name as I don't have access to personal data of individuals. If you need assistance with something specific, feel free to ask!\""},"metadata":{}}]},{"cell_type":"markdown","source":"Conversation Buffer","metadata":{}},{"cell_type":"code","source":"# Create an updated prompt template to include a chat history\ntemplate = \"\"\"<s><|user|>Current conversation:{chat_history}\n\n{input_prompt}<|end|>\n<|assistant|>\"\"\"\n\nprompt = PromptTemplate(\n    template=template,\n    input_variables=[\"input_prompt\", \"chat_history\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:00:35.077995Z","iopub.execute_input":"2024-10-29T11:00:35.078387Z","iopub.status.idle":"2024-10-29T11:00:35.083422Z","shell.execute_reply.started":"2024-10-29T11:00:35.078350Z","shell.execute_reply":"2024-10-29T11:00:35.082549Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from langchain.memory import ConversationBufferMemory\n\n# Define the type of memory we will use\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\n\n# Chain the LLM, prompt, and memory together\nllm_chain = LLMChain(\n    prompt=prompt,\n    llm=llm,\n    memory=memory\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:02:41.980260Z","iopub.execute_input":"2024-10-29T11:02:41.980789Z","iopub.status.idle":"2024-10-29T11:02:42.022466Z","shell.execute_reply.started":"2024-10-29T11:02:41.980755Z","shell.execute_reply":"2024-10-29T11:02:42.021571Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2062104374.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# Generate a conversation and ask a basic question\nllm_chain.invoke({\"input_prompt\": \"Hi! My name is Priyanshu. What is 1 + 1?\"})","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:04:00.858179Z","iopub.execute_input":"2024-10-29T11:04:00.858785Z","iopub.status.idle":"2024-10-29T11:04:16.812702Z","shell.execute_reply.started":"2024-10-29T11:04:00.858745Z","shell.execute_reply":"2024-10-29T11:04:16.811731Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'input_prompt': 'Hi! My name is Priyanshu. What is 1 + 1?',\n 'chat_history': '',\n 'text': \" The sum of 1 + 1 is 2. It's a basic arithmetic calculation!\\n\\nHere's how it looks:\\n\\n1\\n+1\\n---\\n=2\"}"},"metadata":{}}]},{"cell_type":"code","source":"# Does the LLM remember the name we gave it?\nllm_chain.invoke({\"input_prompt\": \"What is my name?\"})","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:33:41.757632Z","iopub.execute_input":"2024-10-29T11:33:41.758479Z","iopub.status.idle":"2024-10-29T11:34:00.706786Z","shell.execute_reply.started":"2024-10-29T11:33:41.758437Z","shell.execute_reply":"2024-10-29T11:34:00.705838Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'input_prompt': 'What is my name?',\n 'chat_history': \"Human: Hi! My name is Priyanshu. What is 1 + 1?\\nAI:  The sum of 1 + 1 is 2. It's a basic arithmetic calculation!\\n\\nHere's how it looks:\\n\\n1\\n+1\\n---\\n=2\",\n 'text': \" My name is you're interacting with the AI, and I don't have a personal name like humans. You can just refer to me as Assistant.\\n\"}"},"metadata":{}}]},{"cell_type":"markdown","source":"Windowed Conversation Buffer","metadata":{}},{"cell_type":"code","source":"from langchain.memory import ConversationBufferWindowMemory\n\n# Retain only the last 2 conversations in memory\nmemory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n\n# Chain the LLM, prompt, and memory together\nllm_chain = LLMChain(\n    prompt=prompt,\n    llm=llm,\n    memory=memory\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:43:52.642992Z","iopub.execute_input":"2024-10-29T11:43:52.643672Z","iopub.status.idle":"2024-10-29T11:43:52.655873Z","shell.execute_reply.started":"2024-10-29T11:43:52.643631Z","shell.execute_reply":"2024-10-29T11:43:52.655029Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1417333225.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# Ask two questions and generate two conversations in its memory\nllm_chain.predict(input_prompt=\"Hi! My name is Priyanshu and I am 22 years old. What is 1 + 1?\")\nllm_chain.predict(input_prompt=\"What is 3 + 3?\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:46:12.477655Z","iopub.execute_input":"2024-10-29T11:46:12.478042Z","iopub.status.idle":"2024-10-29T11:46:48.965578Z","shell.execute_reply.started":"2024-10-29T11:46:12.478004Z","shell.execute_reply":"2024-10-29T11:46:48.964571Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"' The calculation for 3 + 3 is as follows:\\n\\n3\\n+3\\n---\\n6\\n\\nSo, 3 plus 3 equals 6.'"},"metadata":{}}]},{"cell_type":"code","source":"# Check whether it knows the name we gave it\nllm_chain.invoke({\"input_prompt\": \"What is my name?\"})","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:49:05.483201Z","iopub.execute_input":"2024-10-29T11:49:05.483601Z","iopub.status.idle":"2024-10-29T11:49:37.214349Z","shell.execute_reply.started":"2024-10-29T11:49:05.483559Z","shell.execute_reply":"2024-10-29T11:49:37.213356Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'input_prompt': 'What is my name?',\n 'chat_history': \"Human: Hi! My name is Priyanshu and I am 22 years old. What is 1 + 1?\\nAI:  Hi Priyanshu! It's nice to meet you. 1 + 1 equals 2.\\n\\nHere is a simple mathematical computation:\\n\\n1\\n+1\\n---\\n2\\nHuman: What is 3 + 3?\\nAI:  The calculation for 3 + 3 is as follows:\\n\\n3\\n+3\\n---\\n6\\n\\nSo, 3 plus 3 equals 6.\",\n 'text': ' Your name is not explicitly mentioned in our current conversation. However, you introduced yourself as Priyanshu at the beginning of this interaction.\\n\\nAs for your question \"What is my name?\", since it was established that you identified yourself as Priyanshu, I would respond with:\\n\\nYour name, based on this conversation, is Priyanshu.'}"},"metadata":{}}]},{"cell_type":"code","source":"# Check whether it knows the age we gave it\nllm_chain.invoke({\"input_prompt\": \"What is my age?\"})","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:51:02.717485Z","iopub.execute_input":"2024-10-29T11:51:02.718228Z","iopub.status.idle":"2024-10-29T11:52:03.925496Z","shell.execute_reply.started":"2024-10-29T11:51:02.718190Z","shell.execute_reply":"2024-10-29T11:52:03.924530Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'input_prompt': 'What is my age?',\n 'chat_history': 'Human: What is 3 + 3?\\nAI:  The calculation for 3 + 3 is as follows:\\n\\n3\\n+3\\n---\\n6\\n\\nSo, 3 plus 3 equals 6.\\nHuman: What is my name?\\nAI:  Your name is not explicitly mentioned in our current conversation. However, you introduced yourself as Priyanshu at the beginning of this interaction.\\n\\nAs for your question \"What is my name?\", since it was established that you identified yourself as Priyanshu, I would respond with:\\n\\nYour name, based on this conversation, is Priyanshu.',\n 'text': ' I\\'m unable to provide your age as there is no information shared regarding it in our current conversation. Additionally, for privacy reasons, I don\\'t have access to personal data about individuals unless it has been shared with you during our interaction.\\n\\nHowever, if this question was hypothetical and meant to test the AI\\'s response capabilities within a fictional context where your age is known but not discussed yet, then a general answer would be:\\n\\nIn a scenario where we know your age through prior conversation or mutual agreement, you could say, \"My age is [age].\" But in our current interaction, I don\\'t have that information.'}"},"metadata":{}}]},{"cell_type":"markdown","source":"Conversation Summary","metadata":{}},{"cell_type":"code","source":"# Create a summary prompt template\nsummary_prompt_template = \"\"\"<s><|user|>Summarize the conversations and update with the new lines.\n\nCurrent summary:\n{summary}\n\nnew lines of conversation:\n{new_lines}\n\nNew summary:<|end|>\n<|assistant|>\"\"\"\nsummary_prompt = PromptTemplate(\n    input_variables=[\"new_lines\", \"summary\"],\n    template=summary_prompt_template\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:56:56.877519Z","iopub.execute_input":"2024-10-29T11:56:56.877882Z","iopub.status.idle":"2024-10-29T11:56:56.882744Z","shell.execute_reply.started":"2024-10-29T11:56:56.877848Z","shell.execute_reply":"2024-10-29T11:56:56.881766Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from langchain.memory import ConversationSummaryMemory\n\n# Define the type of memory we will use\nmemory = ConversationSummaryMemory(\n    llm=llm,\n    memory_key=\"chat_history\",\n    prompt=summary_prompt\n)\n# Chain the LLM, prompt, and memory together\nllm_chain = LLMChain(\n    prompt=prompt,\n    llm=llm,\n    memory=memory\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:59:32.162755Z","iopub.execute_input":"2024-10-29T11:59:32.163145Z","iopub.status.idle":"2024-10-29T11:59:32.175495Z","shell.execute_reply.started":"2024-10-29T11:59:32.163110Z","shell.execute_reply":"2024-10-29T11:59:32.174620Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/4265320391.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  memory = ConversationSummaryMemory(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Generate a conversation and ask for the name\nllm_chain.invoke({\"input_prompt\": \"Hi! My name is Priyanshu. What is 1 + 1?\"})\nllm_chain.invoke({\"input_prompt\": \"What is my name?\"})","metadata":{"execution":{"iopub.status.busy":"2024-10-29T12:00:53.522579Z","iopub.execute_input":"2024-10-29T12:00:53.522940Z","iopub.status.idle":"2024-10-29T12:02:28.232943Z","shell.execute_reply.started":"2024-10-29T12:00:53.522906Z","shell.execute_reply":"2024-10-29T12:02:28.232041Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"{'input_prompt': 'What is my name?',\n 'chat_history': \" Human: Hi! My name is Priyanshu. What is 1 + 1?\\nAI: The sum of 1 + 1 is 2. It's a basic arithmetic calculation!\\n\\nUpdated Summary: Priyanshu introduces himself and asks the AI about the result of 1 + 1, which equals 2 according to the AI.\",\n 'text': ' Your name is Priyanshu, as mentioned in the current conversation.'}"},"metadata":{}}]},{"cell_type":"code","source":"# Check whether it has summarized everything thus far\nllm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})","metadata":{"execution":{"iopub.status.busy":"2024-10-29T12:04:11.217719Z","iopub.execute_input":"2024-10-29T12:04:11.218106Z","iopub.status.idle":"2024-10-29T12:05:42.558451Z","shell.execute_reply.started":"2024-10-29T12:04:11.218065Z","shell.execute_reply":"2024-10-29T12:05:42.557541Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"{'input_prompt': 'What was the first question I asked?',\n 'chat_history': ' In the updated conversation, Priyanshu introduces himself and inquires about the result of 1 + 1. The AI responds by confirming that 1 + 1 equals 2. Additionally, when asked about his name, the AI reaffirms that it is Priyanshu based on their current conversation.',\n 'text': ' The first question you asked was, \"Priyanshu introduces himself and inquires about the result of 1 + 1.\" However, if we consider this as a dialogue format where Priyanshu is asking rather than introducing himself, then the first question would be: \"What is the result of 1 + 1?\"'}"},"metadata":{}}]},{"cell_type":"code","source":"# Check what the summary is thus far\nmemory.load_memory_variables({})","metadata":{"execution":{"iopub.status.busy":"2024-10-29T12:07:32.007385Z","iopub.execute_input":"2024-10-29T12:07:32.008367Z","iopub.status.idle":"2024-10-29T12:07:32.014069Z","shell.execute_reply.started":"2024-10-29T12:07:32.008324Z","shell.execute_reply":"2024-10-29T12:07:32.013158Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"{'chat_history': ' In the updated conversation, Priyanshu asks about the result of 1 + 1 and learns that it equals 2. He then confirms his name as Priyanshu based on their current interaction. When queried about the first question asked, he refers to it either as his self-introduction or specifically inquiring \"What is the result of 1 + 1?\" depending on interpretation. The AI reaffirms that 1 + 1 equals 2 and acknowledges Priyanshu\\'s name in response to their conversation thus far.'}"},"metadata":{}}]},{"cell_type":"markdown","source":"ReAct in LangChain","metadata":{}},{"cell_type":"code","source":"%pip install langchain_openai","metadata":{"execution":{"iopub.status.busy":"2024-10-29T12:26:33.402813Z","iopub.execute_input":"2024-10-29T12:26:33.403221Z","iopub.status.idle":"2024-10-29T12:26:45.629487Z","shell.execute_reply.started":"2024-10-29T12:26:33.403182Z","shell.execute_reply":"2024-10-29T12:26:45.628382Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Collecting langchain_openai\n  Downloading langchain_openai-0.2.4-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: langchain-core<0.4.0,>=0.3.13 in /opt/conda/lib/python3.10/site-packages (from langchain_openai) (0.3.13)\nRequirement already satisfied: openai<2.0.0,>=1.52.0 in /opt/conda/lib/python3.10/site-packages (from langchain_openai) (1.52.2)\nCollecting tiktoken<1,>=0.7 (from langchain_openai)\n  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.13->langchain_openai) (6.0.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.13->langchain_openai) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.125 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.13->langchain_openai) (0.1.137)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.13->langchain_openai) (24.1)\nRequirement already satisfied: pydantic<3.0.0,>=2.5.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.13->langchain_openai) (2.9.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.13->langchain_openai) (8.3.0)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.13->langchain_openai) (4.12.2)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (0.27.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (0.6.1)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (4.66.4)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.0->langchain_openai) (3.7)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.0->langchain_openai) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->langchain_openai) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->langchain_openai) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->langchain_openai) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.13->langchain_openai) (2.4)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.13->langchain_openai) (3.10.4)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.13->langchain_openai) (1.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.13->langchain_openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.13->langchain_openai) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (1.26.18)\nDownloading langchain_openai-0.2.4-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tiktoken, langchain_openai\nSuccessfully installed langchain_openai-0.2.4 tiktoken-0.8.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom langchain_openai import ChatOpenAI\n\n# Load OpenAI's LLMs with LangChain\nos.environ[\"OPENAI_API_KEY\"] = user_secrets.get_secret(\"openAIAPIKey\")\n\nopenai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T12:27:18.628464Z","iopub.execute_input":"2024-10-29T12:27:18.629170Z","iopub.status.idle":"2024-10-29T12:27:19.152501Z","shell.execute_reply.started":"2024-10-29T12:27:18.629129Z","shell.execute_reply":"2024-10-29T12:27:19.151746Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Create the ReAct template\nreact_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n\n{tools}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: {input}\nThought:{agent_scratchpad}\"\"\"\n\nprompt = PromptTemplate(\n    template=react_template,\n    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T12:33:22.288060Z","iopub.execute_input":"2024-10-29T12:33:22.288917Z","iopub.status.idle":"2024-10-29T12:33:22.293887Z","shell.execute_reply.started":"2024-10-29T12:33:22.288877Z","shell.execute_reply":"2024-10-29T12:33:22.292901Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"%pip install duckduckgo_search","metadata":{"execution":{"iopub.status.busy":"2024-10-29T12:37:00.567959Z","iopub.execute_input":"2024-10-29T12:37:00.568755Z","iopub.status.idle":"2024-10-29T12:37:12.477821Z","shell.execute_reply.started":"2024-10-29T12:37:00.568714Z","shell.execute_reply":"2024-10-29T12:37:12.476811Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Collecting duckduckgo_search\n  Downloading duckduckgo_search-6.3.3-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: click>=8.1.7 in /opt/conda/lib/python3.10/site-packages (from duckduckgo_search) (8.1.7)\nCollecting primp>=0.6.5 (from duckduckgo_search)\n  Downloading primp-0.6.5-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nDownloading duckduckgo_search-6.3.3-py3-none-any.whl (27 kB)\nDownloading primp-0.6.5-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: primp, duckduckgo_search\nSuccessfully installed duckduckgo_search-6.3.3 primp-0.6.5\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.agents import load_tools, Tool\nfrom langchain.tools import DuckDuckGoSearchResults\n\n# You can create the tool to pass to an agent\nsearch = DuckDuckGoSearchResults()\nsearch_tool = Tool(\n    name=\"duckduck\",\n    description=\"A web search engine. Use this to as a search engine for general queries.\",\n    func=search.run,\n)\n\n# Prepare tools\ntools = load_tools([\"llm-math\"], llm=openai_llm)\ntools.append(search_tool)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T12:37:17.607906Z","iopub.execute_input":"2024-10-29T12:37:17.608288Z","iopub.status.idle":"2024-10-29T12:37:17.733267Z","shell.execute_reply.started":"2024-10-29T12:37:17.608252Z","shell.execute_reply":"2024-10-29T12:37:17.732525Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"from langchain.agents import AgentExecutor, create_react_agent\n\n# Construct the ReAct agent\nagent = create_react_agent(openai_llm, tools, prompt)\nagent_executor = AgentExecutor(\n    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T12:39:46.943032Z","iopub.execute_input":"2024-10-29T12:39:46.944105Z","iopub.status.idle":"2024-10-29T12:39:46.962820Z","shell.execute_reply.started":"2024-10-29T12:39:46.944061Z","shell.execute_reply":"2024-10-29T12:39:46.961791Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# What is the price of a MacBook Pro?\nagent_executor.invoke(\n    {\n        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.\"\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T12:41:47.638264Z","iopub.execute_input":"2024-10-29T12:41:47.639005Z","iopub.status.idle":"2024-10-29T12:41:52.460175Z","shell.execute_reply.started":"2024-10-29T12:41:47.638965Z","shell.execute_reply":"2024-10-29T12:41:52.459348Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3mI should use a web search engine to find the current price of a MacBook Pro in USD and then use a calculator to convert it to EUR.\nAction: duckduck\nAction Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: M3 MacBook Air (15-inch): $1,098. Save $200. Amazon is now offering a $100 discount on both the 256GB and 512GB 15-inch M3 MacBook Air, which drops the prices down to $1,099 and $1,299 ..., title: Best MacBook Deals: Up to $500 Off M1, M2 and M3 Models - CNET, link: https://www.cnet.com/deals/best-macbook-deals/, snippet: The MacBook Pro has long been the portable go-to choice for video editors, photographers, and other creative professionals. ... Original Price: $1,499 (USD) Maximum Runtime: Up to 17 hours; Neural Engine Cores: 16; In the Box: 13-inch MacBook Pro, USB-C Charge Cable (2 m), 61W USB-C Power Adapter; Part Number: MYD92LL/A;, title: MacBook Pro 13-inch (M1, 8GB, 512GB) Space Gray, link: https://prices.appleinsider.com/product/m1-macbook-pro-13-inch/MYD92LL/A, snippet: Apple 14\" MacBook Pro M3: $1,599 from $1,499 @ Apple Students, parents, educators, and faculty save $100 on the MacBook Pro M3 series at the Apple Education Store. Prices start from $1,499., title: Best MacBook deals in October 2024: Save up to $500 on MacBook Pro ..., link: https://www.laptopmag.com/deals/best-macbook-deals, snippet: And there could be exclusive additional discounts for My Best Buy members. Today's best MacBook Pro deals in your region. Apple MacBook Pro 14-inch M3 (2023) $1,599. $1,299. View Deal. See all ..., title: The cheapest MacBook Pro deals in October 2024 - TechRadar, link: https://www.techradar.com/news/the-best-cheap-macbook-pro-deals\u001b[0m\u001b[32;1m\u001b[1;3mI have found the current price of a MacBook Pro in USD. Now I will use a calculator to convert it to EUR.\nAction: Calculator\nAction Input: 1499 * 0.85\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 1274.1499999999999\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\nFinal Answer: The current price of a MacBook Pro in USD is $1499. If the exchange rate is 0.85 EUR for 1 USD, it would cost approximately 1274.15 EUR.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.',\n 'output': 'The current price of a MacBook Pro in USD is $1499. If the exchange rate is 0.85 EUR for 1 USD, it would cost approximately 1274.15 EUR.'}"},"metadata":{}}]}]}